{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png\" width=\"100\" align=\"center\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semántica distribuida \n",
    "\n",
    "## Ingeniería de Características \n",
    "\n",
    "### Maestría en Ciencia de Datos\n",
    "### Universidad de Sonora\n",
    "\n",
    "#### Julio Waissman Vilanova (julio.waissman@unison.mx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta libreta vamos a ver como se generan, donde se pueden descargar y algunas aplicaciones los vectores de incrustaciones de palabra (*word embeddings vectors*). En esta libreta vamos a revisar como generar un modelo de vectores de palabras con las dos técnicas más populares. En segundo lugar vamos a revisar como utilizar modelos pre-entrenados, para después ver algunas de las propiedades que parecen *mágicas* en este tipo de modelos. Por último vamos a revisar como una técnica que, a pesar de ser muy simple, es un método muy efectivo para la codificación densa de sentencias o documentos relativamente pequeños."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Generando vectores de palabras con `gensim`.\n",
    "\n",
    "Para esta libreta vamos a utilizar la biblioteca de modelado de tópicos `gensim` de manera intensiva, ya que es la que proporciona las mejores herramientas para el uso de los *word embeddings* (vetores de incrustaciones) o *word vectors* (WV), y que permite utilizar modelos entrenados por otras personas y/o organizaciones. \n",
    "\n",
    "Vamos a generar un modelo de vectores de palabras a partir del texto del Quijote. Este procedimiento se puede aplicar con *corpus* mas grandes, para obtener mejores resultados, pero los tiempos de procesamiento no son aceptables para este curso. Se deja como tarea la obtención de dichos vectores.\n",
    "\n",
    "Primero vamos a cargar el texto, normalizarlo y tokenizarlo. La idea es tener una lista de documentos, los cuales a su vez son una lista de palabras. Para esto vamos a utilizar los métodos que ya hemos desarrollado anteriormente. Como en todo método de PLN, el paso de normalización de texto determina fuertemente la calidad del modelo obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "palabras_vacias = set(nltk.corpus.stopwords.words('spanish'))\n",
    "\n",
    "def normaliza_texto(texto):\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    # Elimina simbolos\n",
    "    texto = texto.replace('\\n', ' ')\n",
    "\n",
    "    # Elimina palabras de paro\n",
    "    texto = ' '.join([palabra for palabra in texto.split() \n",
    "                     if palabra not in palabras_vacias])\n",
    "    \n",
    "    return [nltk.tokenize.regexp_tokenize(frase, r'\\w+') \n",
    "            for frase in nltk.tokenize.sent_tokenize(texto, language='spanish')]\n",
    "\n",
    "\n",
    "archivo = \"quijote.txt\"\n",
    "with open(archivo, 'r', encoding='utf8') as fp:\n",
    "    texto = fp.read()\n",
    "documentos = normaliza_texto(texto)\n",
    "\n",
    "[' '.join(doc) for doc in documentos[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que se obtuvo el corpus, vamos a obtener dos modelos de vectores de palabras diferentes, uno usando el método  desarrollado por *google* ([*word2vec*](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)) y otro por el método desarrollado por *facebook* ([*fastText*](https://arxiv.org/pdf/1607.04606.pdf)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models \n",
    "\n",
    "modelo_w2v = models.Word2Vec(\n",
    "    sentences=documentos,  #  Documentos para el entrenamiento\n",
    "    size=50,              #  tamaño del vector\n",
    "    window=5,              #  tamaño de la ventana del contexto\n",
    "    min_count=5,           #  minima frecuencia para considerar una palabra en el vocabulario\n",
    "    workers=3,             #  número de hilos para multiproceso\n",
    "    sg=0                   #  si 0 usa CBOW, si 1 Skipgrama\n",
    ")\n",
    "\n",
    "modelo_fst = models.FastText(\n",
    "    sentences=documentos,  #  Documentos para el entrenamiento\n",
    "    size=50,              #  tamaño del vector\n",
    "    window=5,              #  tamaño de la ventana del contexto\n",
    "    min_count=5,           #  minima frecuencia para considerar una palabra en el vocabulario\n",
    "    workers=3,             #  número de hilos para multiproceso\n",
    "    sg=0                   #  si 0 usa CBOW, si 1 Skipgrama\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que el *corpus* utilizado es relativamente pequeño, utilizamos un WV de dimensión relativamente pequeño. Lo normal es utilizar un vector entre 200 y 500 dimensiones. Por ejemplo, todos los vectores preentrenados que vamos a utilidar tienen un tamaño de 300, lo que parece ser un tamaño correcto para el español.\n",
    "\n",
    "En este caso no es necesario, pero para un modelo con un corpus mayor, el cual puede durar horas o días de entrenamiento, es muy importante poder guardar el modelo. Cabe aclarar que estos modelos pueden ser *reentrenados* con nuevos documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_w2v.save(\"w2v_quijote.model\")\n",
    "modelo_fst.save(\"fst_quijote.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hagamos algunos experimentos con los modelos entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revisa_palabra(modelo_wv, palabra):\n",
    "    print(\"\\n\\n¿Se encuentra la palabra {} en los wv? {}\".format(palabra, palabra in modelo_wv.vocab))\n",
    "\n",
    "    palabras_similares = modelo_wv.most_similar(positive=palabra, topn=10)\n",
    "    print(\"\\nLas 10 palabras más similares a {}\".format(palabra))\n",
    "    print(\"{:15}{:10}\".format('Palabras', 'Similaridad'))\n",
    "    for (palabra, simil) in palabras_similares:\n",
    "        print(\"{:15}{}\".format(palabra, simil))    \n",
    "\n",
    "print(\"El vector para {} es:\\n{}\".format('dulcinea', modelo_w2v.wv['dulcinea']))\n",
    "revisa_palabra(modelo_w2v.wv, 'dulcinea')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos que pasa si tratamos de usar una palabra que no tengamos en el vocabulario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revisa_palabra(modelo_w2v.wv, 'dulcineea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hagamos lo mismo con el modelo de *fastText*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revisa_palabra(modelo_fst.wv, 'dulcinea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revisa_palabra(modelo_fst.wv, 'dulcineea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y esta es la mayor cualidad del método de *fastText*, que al formarse los vectores de palabra a partir de información de $n$--gramas de letras, es capaz de extrapolar. No solo es útil para palabras mal escritas, si no tambien para palabras completamente fuera del vocabulario. Por supuesto su capacidad de abstraccion depende del contexto. \n",
    "\n",
    "Por ejemplo, si utilizamos una palabra en un contexto que no exista en el *corpus*, encuentra un vector de palabra, pero no significa nada en su contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revisa_palabra(modelo_fst.wv, 'avión')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Importar modelos existentes\n",
    "\n",
    "Entrenar un modelo a partir de un *corpus* grande es una tarea que toma un tiempo de computo importante. Una opción común es es descargar y usar un modelo ya entrenado por alguien más (dandole el crédito correspondiente). \n",
    "\n",
    "Existen dos maneras de obtener los modelos: el archivo binario con el modelo completo, y la otra es el uso de los llamados [*keyed vectors*](https://radimrehurek.com/gensim/models/keyedvectors.html#why-use-keyedvectors-instead-of-a-full-model). Los modelos con llaves son archivos de texto (típicamente con terminacón `.vec`) en los que en cada renglón se encuentra la palabra, seguida por los valores de su vector de inserción, separados por tabulaciones. El tamaño de los archivos es sensiblemente menos y por lo tanto se utilizan bastante. Además, es posible limitar el número de palabras a utilizar (por ejemplo, las primeras 100,000).  En su contra, tienen que no se puede utilizar el modelo completo (y generalizar a cualquier palabra como en *FastText*. En esta libreta importaremos solamente este tipo de representación.\n",
    "\n",
    "Los modelos entrenados para español disponibles son:\n",
    "\n",
    "1. [**Spanish Billion Word Corpus and Embeddings**](https://crscardellino.github.io/SBWCE/). El [autor](https://crscardellino.github.io/) compilo y normalizó una gran cantidad de texto. En su página se puede descargar el modelo entrenado con el método `word2vec`. Si se utiliza este modelo (u otro modelo entrenado con este *corpus* es necesario citarlos como *Cristian Cardellino: Spanish Billion Words Corpus and Embeddings (March 2016), https://crscardellino.github.io/SBWCE/*.\n",
    "\n",
    "2. [**Spanish Word Embeddings**](https://github.com/uchile-nlp/spanish-word-embeddings). Página del grupo de PLN de la Universidad de Chile. Presenta una lista de modelos, aunque algunos (como el de *Facebook*) se encuentran desactualizados. En la página vienen dos modelos entrenados con el *corpus* SBWC (ver inciso anterior) utilizando los métodos de *GloVe* y *fastText* entrenados por [Jorge Pérez](https://users.dcc.uchile.cl/~jperez/) de la Universidad de Chile.\n",
    "\n",
    "3. [**fastText word vectors for 157 languages**](https://fasttext.cc/docs/en/crawl-vectors.html). *Facebook* pone a disposición de manera abierta modelos entrenados principalmente con los datos de *wikipedia* en 157 idiomas diferentes. Es posible descargar únicamente el modelo en forma de *keyed vectors* o el modelo binario.\n",
    "\n",
    "Los cuatro modelos se encuentran previamente descargados en la imágen de Docker del grupo por lo que no es necesario hacerlo. Vamos a cargar alguno de los modelos para utilizarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Ejecuta esta linea solo si no tienes ya descomprimido el modelo\n",
    "# porque se puede perder bastante tiempo :-)\n",
    "!gunzip -k fasttext-sbwc.vec.gz\n",
    "archivo_sbwc_fst = \"fasttext-sbwc.vec\"\n",
    "\n",
    "# Estos modelos los tienes que descargar tu mismo,\n",
    "# si lo hacía quedaba muy grande el proyecto\n",
    "# escogí el modelo FastText entrenado con el corpus SBWC solamente\n",
    "# porque era el archivo de menor tamaño.\n",
    "\n",
    "# archivo_wiki_fst = \"modelos/fasttext-fcb-es.vec\"\n",
    "# archivo_sbwc_glo = \"modelos/glove-sbwc.vec\"\n",
    "# archivo_sbwc_w2v = \"modelos/word2vec-sbwc.vec\"\n",
    "\n",
    "num_paabras = 100000\n",
    "modelo_wv = KeyedVectors.load_word2vec_format(archivo_sbwc_fst, limit=num_paabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y veamos algunos ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revisa_palabra(modelo_wv, 'dulcinea')\n",
    "revisa_palabra(modelo_wv, 'avión')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. La ecuación *rey* - *hombre* + *mujer* $\\approx$ *reina* y otras analogías\n",
    "\n",
    "Una característica interesante de los vectores de palabras, es que estos capturan muchos tipos de similaridades, los cuales se pueden obtener a partir de operaciones aritméticas entre vectores. Por ejemplo, definamos la similaridad entre dos vectores $u$ y $v$ a través de la *similiaridad coseno* definida como:\n",
    "\n",
    "$$\n",
    "sim(u, v) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|}\n",
    "$$\n",
    "\n",
    "que es equivalente al calculo del coseno del angulo entre ambos vectores. \n",
    "\n",
    "Supongamos que ahora tenemos un par de vectores $a$, $a^*$, donde $a$ es el vector de una palabra y $a*$ el de otra que tiene cierta relación con $a$. Es interesante que el vector resultante de la diferencia entre ambas palabras $a - a^*$ codifica (en algunos casos únicamente) un tipo de relación entre ambas que puede extrapolarse a otras palabras. \n",
    "\n",
    "Si tenemos otra palabra *positiva* $b$, debería existir una palabra $b^*$ que codifique la misma relación. Esto se puede encontrar encontrando $b^*$ tal que la similaridad entre diferencias sea máxima, esto es:\n",
    "\n",
    "$$\n",
    "b^* = \\arg \\max_{b^*\\in V}(\\cos(a - a^*, b - b^*))\n",
    "$$\n",
    "\n",
    "y esto se calcula como\n",
    "\n",
    "$$\n",
    "b^* = \\arg \\max_{b^*\\in V}(\\cos(b^*, b - a + a^*))\n",
    "$$\n",
    "\n",
    "donde $a^*$ y $b$ se conocen como vectores *positivos* y $a$ como vector *negativo* (por sus operaciones aritméticas).\n",
    "\n",
    "Veamos algunas de esas reaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_wv.most_similar(positive=['rey', 'mujer'], negative=['hombre'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_wv.most_similar(positive=['comiendo', 'jugar'], negative=['comer'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_wv.most_similar(positive=['hermosillo', 'españa'], negative=['méxico'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_wv.most_similar(positive=['tacos', 'colombia'], negative=['méxico'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_wv.most_similar(positive=['oaxaca', 'argentina'], negative=['mexico'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo hay que tener mucho cuidado con estas relaciones ya que no hay ninguna base para pensar que siempre funcionan. En general con singular/plural, genero, paises/ciudades y conjugaciones funciona relativamente bien.\n",
    "\n",
    "Tambien hay funciones para otros tipo de análisis de similaridad. Veamos unos ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_wv.doesnt_match(['asado','macarrones','tacos','empanadas', 'pizza', 'frijoles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_wv.closer_than('policía', 'ladrón')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_wv.most_similar_to_given('guadalajara', ['lima', 'córdoba', 'managua', 'rosario', 'medellín'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Vectores de palabras para codificar parágrafos\n",
    "\n",
    "Algo que es interesante es como representar un documento (o párrafo, o frase) a partir de un modelo de palabras. De esa manera podríamos tener una codificación densa de cada documento en un número preestablecido de dimensiones. Entre los diferentes métodos que se han probado, uno que es bastante simple y efectivo (tomando en cuenta desconocimiento del contexto) para documentos pequeños (como entradas de *twitter*), es el de tomar los vectores de palabras de cada una de las palabras de la frase (normalizadas y sin palabras de paro) y sacar el promedio de dichos vectores. Esto nos da un vector de las mismas dimensiones que el vector de cada una de las palabras.\n",
    "\n",
    "Y, así, este vector extrae características de cada palabra, tomando en cuenta el contexto, y utilizando un espacio de representación de una dimensión sensiblemente menor que la que tiene la representación BOW o TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def frase_a_vec(frase, modelo_wv):\n",
    "    N = 0\n",
    "    acc = np.zeros(modelo_wv.vector_size)\n",
    "    for palabra in frase:\n",
    "        if palabra in modelo_wv.vocab:\n",
    "            acc += modelo_wv[palabra]\n",
    "            N += 1\n",
    "    return acc if N==0 else acc/N\n",
    "\n",
    "def sentencias_a_vec(frases, modelo_wv):\n",
    "    vectores = np.zeros((len(frases), modelo_wv.vector_size))\n",
    "    for (i, frase) in enumerate(frases):\n",
    "        vectores[i,:] = frase_a_vec(frase, modelo_wv)\n",
    "    return vectores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar con algunas frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = \"\"\" \n",
    "Aunque nadie puede volver atrás y hacer un nuevo comienzo, cualquiera puede comenzar a partir de ahora y crear un nuevo final.\n",
    "Mira en el nuevo día como otro regalo especial de su creador, otra oportunidad de oro.\n",
    "La felicidad no puede ser ganada, es la experiencia espiritual de vida de cada minuto con amor, gracia y gratitud.\n",
    "Date cuenta que tú eres quien va a llegar a donde quiere ir, nadie más.\n",
    "Si tú no construyes tu sueño, alguien va a contratarte para que le ayudes a construir el suyo. \n",
    "Desearía poder hablarte, desearía poder sonreírte, desearía poder abrazarte, pero sobre todo desearía poder besarte.\n",
    "Tus ojos son como dos lunas, y yo siempre quise viajar a la luna, ahora entiendo mi deseo de ser astronauta.\n",
    "Unos ojos que jamás me cansaré de mirar, unos labios que siempre querré besar, pero lo mejor de todo, un corazón que jamás dejaré de amar.\n",
    "Eres un lindo paisaje con el que me quiero deleitar, tus ojos las flores, tu rostro la pradera y tu boca el mar.\n",
    "Nunca antes se había visto un amor igual al que siento por ti; no cabe en mi corazón, ni tampoco en este universo.\n",
    "Lo único de que me arrepiento en esta vida es que no soy otro.\n",
    "Siempre recuerda que tú eres absolutamente único y especial, tal como todos los demás. \n",
    "Un famoso estudio afirma que de cada 10 millones de personas que escuchan la radio, 5 millones son la mitad.\n",
    "Sé que existe un mundo mucho mejor donde se cumplen todos los deseos, pero es un mundo muy caro.\n",
    "Lo importante en la vida no es tener el privilegio de saber, si no de tener Google como página de inicio en tu movil.\n",
    "\"\"\"\n",
    "\n",
    "sentencias = normaliza_texto(frases)\n",
    "vectores_frases = sentencias_a_vec(sentencias, modelo_wv)\n",
    "\n",
    "# Ahora vamos a ver la similaridad de una frase resecto otras\n",
    "# frase = \"Siento que te debo la canción más bonita del mundo, el dibujo más bello, el poema, así como frases de amor hermosas\"\n",
    "# frase = \"Si amas a algo, dejalo libre, si regresa es tuyo y si no, núnca lo fué\"\n",
    "frase = \"Mi billetera es como una cebolla, abrirla me hace llorar.\"\n",
    "\n",
    "frase_tok = regexp_tokenize(frase.lower(), r'\\w+')\n",
    "v_frase = frase_a_vec(frase_tok, modelo_wv)\n",
    "\n",
    "sim_cos = modelo_wv.cosine_similarities(v_frase, vectores_frases)\n",
    "\n",
    "print(\"Las frases, ordenadas por su similitud con a la frase\\n<<{}>>\\n\".format(' '.join(frase_tok)))\n",
    "for i in sim_cos.argsort()[-1::-1]:\n",
    "    print(\"({:4.3})  {}\".format(sim_cos[i], ' '.join(sentencias[i])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

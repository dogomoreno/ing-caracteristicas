{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png\" width=\"100\" align=\"center\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de documentos \n",
    "\n",
    "## Ingeniería de Características \n",
    "\n",
    "### Maestría en Ciencia de Datos\n",
    "### Universidad de Sonora\n",
    "\n",
    "#### Julio Waissman Vilanova (julio.waissman@unison.mx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta libreta vamos a aplicar las herramientas necesarias para el tratamiento de documentos cuando la secuencia de *tokens* no es importante, al menos en el largo plazo. Estas técnicas tienen la ventaja de ser más rápidas de implementar, y para la clasificación de documentos suele dar buenos resultados. Es por esta razón que un método como este *siempre* debe de aplicarse en un principio como un resultado mínimo aceptable. \n",
    "\n",
    "Vamos a revisar en primer lugar los métodos de extracción de características. En segundo lugar, aplicaremos os métodos usuales para la clasificación de documentos. Por último, vamos a discutir sobre el uso de representaciones densas, así como el uso de métodos basados en aprendizaje profundo para la clasificación de documentos.\n",
    "\n",
    "Vamos a procesar la información sobre el problema de análisis de sentimiento de *TASS 2015*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as et\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (20, 12)\n",
    "\n",
    "# Para poder convertir los tópicos en un problema de multiple clasificación\n",
    "from ast import literal_eval \n",
    "\n",
    "\n",
    "def lee_datos_TASS2015(archivo, etiquetado=True):\n",
    "    arbol = et.parse(archivo)\n",
    "    raiz = arbol.getroot()\n",
    "    data_dic = []\n",
    "    for tweet in raiz.iter('tweet'):\n",
    "        contenido = tweet.find('content').text\n",
    "        if contenido is not None:\n",
    "            data_dic.append({\n",
    "                'texto': contenido,\n",
    "                'id': tweet.find('tweetid').text,\n",
    "                'usuario': '@' + tweet.find('user').text,\n",
    "                'fecha': tweet.find('date').text,               \n",
    "            })\n",
    "            if etiquetado:\n",
    "                data_dic[-1]['polaridad'] = tweet.find('sentiments')[0].find('value').text\n",
    "                data_dic[-1]['tópicos'] = '[' + ', '.join(\n",
    "                    ['\"' + t.text + '\"' for t in tweet.find('topics')]) + ']'\n",
    "    return pd.DataFrame.from_dict(data_dic)\n",
    "\n",
    "df_train = lee_datos_TASS2015(\"general-tweets-train-tagged.xml\") \n",
    "df_test = lee_datos_TASS2015(\"general-tweets-test.xml\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y ahora vamos a limpiar los datos usando expresiones regulares y steaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "usuarios_re = re.compile(r\"@[\\w\\d]+\")\n",
    "hashtags_re = re.compile(r\"#[\\w\\d]+\")\n",
    "email_re = re.compile(r\"\\b[\\w][\\w\\.-]*@\\w[\\w\\.-]*\\.[a-zA-Z]{2,6}\\b\")\n",
    "url_re = re.compile(r\"\\b(\\w+:\\/{2})?[\\d\\w-]+(\\.[\\d\\w-]+)+(/\\S+)*\\b\") \n",
    "remplaza_por_espacios_re = re.compile('[\\n/(){}\\[\\]\\|@,;\\.]')\n",
    "simbolos_a_eliminar_re = re.compile('[^\\d\\w #+_]')\n",
    "palabras_paro = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "def prepara_texto(texto):\n",
    "    text = texto.lower()\n",
    "    \n",
    "    # Codificaciones (problemas con UTF-8, latin1, etc...)\n",
    "    text = re.sub(r'\\\\\\\\', r'\\\\', text)\n",
    "    text = re.sub(r'\\\\\\\\', r'\\\\', text)\n",
    "    text = re.sub(r'\\\\x\\w{2,2}', ' ', text)\n",
    "    text = re.sub(r'\\\\u\\w{4,4}', ' ', text)\n",
    "    text = re.sub(r'\\\\n', ' . ', text)\n",
    "\n",
    "    # Cambia e_mails, urls y usuarios por palabra clave\n",
    "    text = re.sub(email_re, '_EMAIL_', text)\n",
    "    text = re.sub(url_re, '_URL_', text)\n",
    "    text = re.sub(usuarios_re, '_USR_', text)\n",
    "    text = re.sub(hashtags_re, '_HASHTAG_', text)\n",
    "    \n",
    "    # Las palabras con letras repetidas más de 3 veces \n",
    "    # (dos veces por las personas que abusan demasiado)\n",
    "    text = re.sub(r'([a-zA-Z])\\1\\1+(\\w*)', r'\\1\\1\\2', text)\n",
    "    text = re.sub(r'([a-zA-Z])\\1\\1+(\\w*)', r'\\1\\1\\2', text)\n",
    "    \n",
    "    # Elimina simbolos\n",
    "    text = re.sub(remplaza_por_espacios_re, ' ', text)\n",
    "    text = re.sub(simbolos_a_eliminar_re, '', text)\n",
    "    \n",
    "    # Palabras de paro\n",
    "    text = [palabra for palabra in text.split(' ') if palabra not in palabras_paro]\n",
    "    return ' '.join(text)\n",
    "\n",
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "stemmer = nltk.stem.snowball.SpanishStemmer()\n",
    "def stem_texto(texto):\n",
    "    return \" \".join(stemmer.stem(token) for token in tokenizer.tokenize(texto))\n",
    "\n",
    "df_train['texto procesado'] = df_train['texto'].apply(prepara_texto)\n",
    "df_train['texto stem'] = df_train['texto procesado'].apply(stem_texto)\n",
    "display(df_train.head())\n",
    "\n",
    "df_test['texto procesado'] = df_test['texto'].apply(prepara_texto)\n",
    "df_test['texto stem'] = df_test['texto procesado'].apply(stem_texto)\n",
    "display(df_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui se puede escoger entre 'texto', 'texto procesado' y 'texto stem'\n",
    "columna = 'texto'\n",
    "x_train = df_train[columna].values\n",
    "x_test = df_test[columna].values\n",
    "\n",
    "y_polaridad = df_train['polaridad'].values\n",
    "y_topico = df_train['tópicos'].apply(literal_eval).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Extracción de características\n",
    "\n",
    "\n",
    "La extracción de características, sin un método de entrenamiento, solamente puede hacerse con una matriz de características dispersas de dimensión $N \\times V$, donde $N$ es el número de documentos y $V$ es el tamaño del vocabulario. Esto es debido al desconocimiento de las relaciones de las palabras entre sí. Esta no solo es una matriz gigantésca, si no que aparte la gran mayoría de las entradas de la matriz van a ser cero.\n",
    "\n",
    "El método de base es el conocido como *bolsa de palabras (BOW por Bag of words)*, en el cual se asigna un valor de 1 a cada palabra que se encuentre *al menos* una vez en un documento. Como el uso de palabras solas implica que no existe ninguna relación entre palabras, la ampliación del vocabulario a bigramas, trigramas, o $n$-gramas de cualquier orden, permite conservar en cierta forma las relaciones secuenciales más simples, pero a costa de un incremento en forma exponencial de nuestro vocabulario. Vamos a extraer características tanto con unigramas únicamente como con bigramas y unigramas. ¿Qué pasa si se mueven algunos parámetros como `min_df` o `token_pattern`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bog_unigramas_vectorizer = CountVectorizer(\n",
    "    analyzer='word',        #  Separa por palabras (puede ser por caracteres)\n",
    "    binary=True,            #  BOG (si False, entonces cuenta numero de ocurrencias por documento)\n",
    "    ngram_range=(1,1),      #  n-gramas (min, max) a utilizarse, en este caso solo unigramas\n",
    "    token_pattern=r'(\\S+)', #  Patron de tokenización. Por default solo letras y al menos dos letras\n",
    "    min_df=1,               #  Número de documentos (o proporción) mínimo donde un termino valido aparece\n",
    "    max_df=0.9,             #  Número de documentos (o proporción) máximo donde un termino válido aparece\n",
    ")\n",
    "\n",
    "bog_bigramas_vectorizer = CountVectorizer(\n",
    "    binary=True, ngram_range=(1,2), token_pattern=r'(\\S+)', min_df=1, max_df=0.9\n",
    ")\n",
    "\n",
    "x_train_bow1 = bog_unigramas_vectorizer.fit_transform(x_train)\n",
    "x_text_bow1 = bog_unigramas_vectorizer.transform(x_test)\n",
    "\n",
    "x_train_bow2 = bog_bigramas_vectorizer.fit_transform(x_train)\n",
    "x_test_bow2 = bog_bigramas_vectorizer.transform(x_test)\n",
    "\n",
    "print(\"El objeto x_train_bow1 es del tipo{}\".format(type(x_train_bow1)))\n",
    "print(\"La dimensión de x_train_bow1 es {}\".format(x_train_bow1.shape))\n",
    "print(\"La dimensión de x_train_bow2 es {}\".format(x_train_bow2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf–idf) (*term frequency–inverse document frequency*) es una medida de la importancia discriminante de una palabra en un documento perteneciente a un corpus. Esta cantidad se forma por dos componentes:\n",
    "\n",
    "* *TF* (frecuencia del término), describe qué tanto se emplea un término en un documento. La lógica de este componente es que mientras más veces aparece un término en un documento, más representativo es del mismo. Existen diversa maneras de calcular este valor, siendo la más simple el conteo directo del término en el documento. La mayoría de las opciones, sin embargo, utilizan alguna variante basada en la frecuencia relativa del término en el documento. \n",
    "\n",
    "$$\n",
    "tf(t, d) = \\frac{\\textrm{Número de veces que aparece el término }t\\textrm{ en el documento }d}\n",
    "{\\textrm{Número de términos en el documento }d} \n",
    "$$\n",
    "\n",
    "* *IDF* (frecuencia inversa del documento), representa la frecuencia con que es empleado el término en el corpus. Un término que es utilizado con mucha frecuencia en el contexto, es poco  discriminante. La forma básica de calcular esta cantidad es como el inverso de la frecuencia de documentos en que aparece el término (escalado logarítmicamente).\n",
    "\n",
    "$$\n",
    "idf(t, D) = \\log\\frac{ 1 + \\textrm{Número de documentos en el corpus }D}\n",
    "{1 + \\textrm{Número de documentos en el corpus }D\\textrm{ donde aparece el término t}} + 1\n",
    "$$\n",
    "\n",
    "El valor de TF para un término/palabra es específico para cada documento, mientras que IDF es un valor global del término en el corpus. El valor de TF-IDF se obtiene multiplicando los valores de TF e IDF:\n",
    "\n",
    "$$\n",
    "tf\\_idf(t, d, D) = tf(t, d)\\cdot idf(t, D)\n",
    "$$\n",
    "y al final cada renglón es normalizado respecto a todos los términos, de forma que cada documento sea representado por un vector de características con norma unitaria (lo que es particularmente interesante para el entrenamiento de muchos modelos de aprendizaje automático).\n",
    "\n",
    "Este método de extracción de características es una modificación al BOG y por lo tanto, la clase para el vectorizador TF-IDF es básicamente la misma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_unigramas_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,1),      #  n-gramas (min, max) a utilizarse, en este caso solo unigramas\n",
    "    token_pattern=r'(\\S+)', #  Patron de tokenización. Por default solo letras y al menos dos letras\n",
    "    min_df=5,               #  Número de documentos (o proporción) mínimo donde un termino valido aparece\n",
    "    max_df=0.9,             #  Número de documentos (o proporción) máximo donde un termino válido aparece\n",
    "    norm='l2'               #  Norma del vector de cada palabra, lo mñas típico es la euclidiana ('l2')\n",
    ")\n",
    "\n",
    "tfidf_bigramas_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,2), token_pattern=r'(\\S+)', min_df=5, max_df=0.9\n",
    ")\n",
    "\n",
    "x_train_tfidf1 = tfidf_unigramas_vectorizer.fit_transform(x_train)\n",
    "x_text_tfidf1 = tfidf_unigramas_vectorizer.transform(x_test)\n",
    "\n",
    "x_train_tfidf2 = tfidf_bigramas_vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf2 = tfidf_bigramas_vectorizer.transform(x_test)\n",
    "\n",
    "print(\"El objeto x_train_tfidf1 es del tipo{}\".format(type(x_train_tfidf1)))\n",
    "print(\"La dimensión de x_train_tfidf1 es {}\".format(x_train_tfidf1.shape))\n",
    "print(\"La dimensión de x_train_tfidf2 es {}\".format(x_train_tfidf2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir del objeto vectorizador generado (en esta caso `tfidf_bigramas_vectorizer`) se puede obtener información adicional. En particular es importante conocer a que token corresponde cada una de las columnas y viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario = tfidf_bigramas_vectorizer.get_feature_names()\n",
    "\n",
    "for i in [1, 3, 100, 1000, 2000]:\n",
    "    print('La palabra en la posición {} es \"{}\"'.format(i, vocabulario[i]))\n",
    "\n",
    "print('\\nLa palabra \"amor\" se encuentra en la posición {}'.format(tfidf_bigramas_vectorizer.vocabulary_.get('amor')))\n",
    "\n",
    "numdoc = 743\n",
    "a = x_train_tfidf2[numdoc, :].toarray().ravel()\n",
    "mejores = [(vocabulario[i], a[i]) for i in a.argsort()[-1:-4:-1]]\n",
    "\n",
    "print('\\nEl documento {} es:\\n\\t\"{}\"'.format(numdoc, x_train[numdoc]))\n",
    "print('y los 3 tokens con mayor valor de tf-idf son:')\n",
    "for (token, tfidf) in mejores:\n",
    "    print(\"\\t{}\\t\\tvalor de ttf-idf: {:06.4f}\".format(token, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, para la clasificación de textos el método de TF-IDF da mejores resultados que BOW, y su complejidad computacional es muy similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Clasificación de documentos\n",
    "\n",
    "En esta sección vamos a mostrar dos formas diferentes de clasificar documentos, utilizando los métodos disponibles en *sklearn*: Una para una clasificación en la que cada documento del conjunto de aprendizaje se encuentra etiquetado en una sola clase (como es el caso de la polaridad, para el análisis de sentimientos), y el caso en el cual cada documento puede estar asignado a una o más etiquetas (como es el caso de los tópicos).\n",
    "\n",
    "Al estar fuera del alcance de este curso la discusión sobre los métodos de aprendizaje supervisado, vamos aretener solamente uno de los métodos más clásicos (regresión logística). Les recomiendo explorar otros métodos, en particular las máquinas de vectores de soporte para clasificación y los bosques aleatorios. Estos dos métodos suelen obtener muy buenos resultados en los diferentes *benchmarks* y competencias.\n",
    "\n",
    "### 3.2.1 Análisis de sentimientos (polaridad)\n",
    "\n",
    "Comencemos por el principio, que es separar los datos de entrenamiento de los de validación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_entrena, x_valida, y_entrena, y_valida = train_test_split(\n",
    "    x_train_tfidf2, \n",
    "    y_polaridad, \n",
    "    test_size=0.1, \n",
    "    random_state=10\n",
    ")\n",
    "print('Tamaño del conjunto de entrenamiento = {}. \\\n",
    "       Tamaño del conjunto de validación = {}.'.format(x_entrena.shape[0], x_valida.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a generar un objeto clasificador, con un [clasificador logístico](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), y realizamos el entrenamiento. Para más información sobre la regresión logística un buen tutorial se puede encontrar [aquí (capítulo 2, pero el capítulo 1 tambien está muy bueno](http://cs229.stanford.edu/notes/cs229-notes1.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    penalty='l2',          # Norma a seguir en la optimización ('l2' o 'l1')\n",
    "    C=1.0,                 # Inverso a la constante de regularización, para mejorar la generalización\n",
    "    random_state=1,        # Solo para poder tener repetibilidad (muy útil para debuguear)\n",
    "    multi_class='ovr'      # Para más de dos clases ('ovr' para one-vs-all, 'multinomial' para softmax)\n",
    ")\n",
    "\n",
    "clf.fit(x_entrena, y_entrena)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a ver los resultados, de acuerdo a los [indicadores clásicos](https://en.wikipedia.org/wiki/Precision_and_recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def reporte(clasificador, x, t, labels=None):\n",
    "    y = clasificador.predict(x)\n",
    "    print(\"\\nPorcentaje de acierto: {}\".format(accuracy_score(t, y)))\n",
    "    print(\"\\nPrecisión, recall y f1-score\")\n",
    "    print(classification_report(t, y, target_names=labels))    \n",
    "\n",
    "print(\"\\n\\nPara los datos de entrenamiento\\n\" + 40*\"=\")\n",
    "reporte(clf, x_entrena, y_entrena)\n",
    "print(\"\\n\\nPara los datos de validación\\n\" + 40*\"=\")\n",
    "reporte(clf, x_valida, y_valida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los [resultados reportados](http://ceur-ws.org/Vol-1397/overview.pdf) con el conjunto de prueba (del que no disponemos los datos) se encuentran distribuidos alrededor del 0.5 de porcentaje de acierto. Es interesante notar como nuestros resultados no son tan alejados (en un primer intento) de los presentados en trabajos que hacen uso de tecnicas extremadamente sofisticadas, como son las [redes neuronales profundas recurrentes](http://ceur-ws.org/Vol-1397/lys.pdf). Sin embargo, no estamos conformes con los resultados y debemos de hacer una mejor clasificación, lo que nos lleva a la *tarea 1*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, es importante la interpretabilidad del modelo. Dado que en la regresión logística se agrega la información de cada una de las características de forma lineal, es posible ver a los coeficientes como una medida de peso de la importancia (positiva y negativa) que tienen cada uno de los *tokens* para cada una de las etiquetas. Veamos cuales son las palabras con más peso positivo y con más peso negativo para una etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_principales(clasificador, clase, vocabulario):\n",
    "    clase_ind = clasificador.classes_.tolist().index(clase)\n",
    "    pesos = clasificador.coef_[clase_ind,:]\n",
    "    indices = pesos.argsort()\n",
    "    mejores = [(vocabulario[i], pesos[i]) for i in indices[-1:-6:-1]]\n",
    "    peores = [(vocabulario[i], pesos[i]) for i in indices[:5]]\n",
    "    \n",
    "    return mejores, peores\n",
    "\n",
    "clase_sel = 'N+'\n",
    "mejores, peores = tokens_principales(clf, clase_sel, vocabulario)\n",
    "\n",
    "print(\"Las clases disponibles son: {}\".format(clf.classes_.tolist()))\n",
    "print(\"Los tokens más positivos para la clase {} son:\".format(clase_sel))\n",
    "for (t, w) in mejores:\n",
    "    print(\"\\t- {:25}(peso:{:6.4})\".format(t,w))\n",
    "print(\"Los tokens más negativos para la clase son:\")\n",
    "for (t, w) in peores:\n",
    "    print(\"\\t- {:25}(peso:{:6.4})\".format(t,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, veamos que encuentra con el conjunto de prueba, revisando algunos ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = clf.predict(x_test_tfidf2)\n",
    "\n",
    "for i in [1, 10, 100, 500]:\n",
    "    print(\"Twwet:\\t {}\".format(df_test.loc[i,'texto']))\n",
    "    print('Polaridad estimada: {}'.format(y_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Clasificación de tópicos\n",
    "\n",
    "Ahora vamos a realizar la clasificación de tópicos, la cual es una tarea más dificil que el análisis de sentimientos, al estár más clases involucradas, y al tener varias etquetas por cada documento. Para esto vamos a utilizar los métodos del módulo *sklearn*. Vamos a preparar los datos para realizar la tarea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Se procesa las etiquetas en forma de multietiquetas\n",
    "topico_count = {}\n",
    "for y in y_topico:\n",
    "    for topico in y:\n",
    "        topico_count[str(topico)] = topico_count.setdefault(topico, 0) + 1    \n",
    "mlb = MultiLabelBinarizer(classes=sorted(topico_count.keys()))\n",
    "y_topico_mlb = mlb.fit_transform(y_topico)\n",
    "\n",
    "# Se separan los datos en conjunto de entrenamiento y validación\n",
    "x_ent_mlb, x_val_mlb, y_ent_mlb, y_val_mlb = train_test_split(\n",
    "    x_train_tfidf2, \n",
    "    y_topico_mlb, \n",
    "    test_size=0.1, \n",
    "    random_state=10\n",
    ")\n",
    "\n",
    "# Se crea un clasificador para multiples etiquetas\n",
    "clf_mlb = OneVsRestClassifier(\n",
    "    LogisticRegression(\n",
    "        C=5.0,\n",
    "        penalty='l2'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Se entrena el clasificador\n",
    "clf_mlb.fit(x_ent_mlb, y_ent_mlb)\n",
    "\n",
    "# Se revisa los indices de desempeño\n",
    "print(\"\\n\\nPara los datos de entrenamiento\\n\" + 40*\"=\")\n",
    "reporte(clf_mlb, x_ent_mlb, y_ent_mlb, mlb.classes)\n",
    "print(\"\\n\\nPara los datos de validación\\n\" + 40*\"=\")\n",
    "reporte(clf_mlb, x_val_mlb, y_val_mlb, mlb.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para terminar la sección, revisemos algunos tweets del conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_mlb = clf_mlb.predict(x_test_tfidf2)\n",
    "y_test_topico = mlb.inverse_transform(y_test_mlb)\n",
    "\n",
    "for i in [1, 10, 100, 500, 1000, 2000]:\n",
    "    print(\"Twwet:\\t {}\".format(df_test.loc[i,'texto']))\n",
    "    print('Tópicos estimados: {}'.format(y_test_topico[i]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

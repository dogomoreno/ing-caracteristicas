{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png\" width=\"100\" align=\"center\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de documentos \n",
    "\n",
    "## Ingeniería de Características \n",
    "\n",
    "### Maestría en Ciencia de Datos\n",
    "### Universidad de Sonora\n",
    "\n",
    "#### Julio Waissman Vilanova (julio.waissman@unison.mx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta libreta vamos a aplicar las herramientas necesarias para el tratamiento de documentos cuando la secuencia de *tokens* no es importante, al menos en el largo plazo. Estas técnicas tienen la ventaja de ser más rápidas de implementar, y para la clasificación de documentos suele dar buenos resultados. Es por esta razón que un método como este *siempre* debe de aplicarse en un principio como un resultado mínimo aceptable. \n",
    "\n",
    "Vamos a revisar en primer lugar los métodos de extracción de características. En segundo lugar, aplicaremos os métodos usuales para la clasificación de documentos. Por último, vamos a discutir sobre el uso de representaciones densas, así como el uso de métodos basados en aprendizaje profundo para la clasificación de documentos.\n",
    "\n",
    "Vamos a procesar la información sobre el problema de análisis de sentimiento de *TASS 2015*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as et\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (20, 12)\n",
    "\n",
    "# Para poder convertir los tópicos en un problema de multiple clasificación\n",
    "from ast import literal_eval \n",
    "\n",
    "\n",
    "def lee_datos_TASS2015(archivo, etiquetado=True):\n",
    "    arbol = et.parse(archivo)\n",
    "    raiz = arbol.getroot()\n",
    "    data_dic = []\n",
    "    for tweet in raiz.iter('tweet'):\n",
    "        contenido = tweet.find('content').text\n",
    "        if contenido is not None:\n",
    "            data_dic.append({\n",
    "                'texto': contenido,\n",
    "                'id': tweet.find('tweetid').text,\n",
    "                'usuario': '@' + tweet.find('user').text,\n",
    "                'fecha': tweet.find('date').text,               \n",
    "            })\n",
    "            if etiquetado:\n",
    "                data_dic[-1]['polaridad'] = tweet.find('sentiments')[0].find('value').text\n",
    "                data_dic[-1]['tópicos'] = '[' + ', '.join(\n",
    "                    ['\"' + t.text + '\"' for t in tweet.find('topics')]) + ']'\n",
    "    return pd.DataFrame.from_dict(data_dic)\n",
    "\n",
    "df_train = lee_datos_TASS2015(\"general-tweets-train-tagged.xml\") \n",
    "df_test = lee_datos_TASS2015(\"general-tweets-test.xml\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y ahora vamos a limpiar los datos usando expresiones regulares y steaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>id</th>\n",
       "      <th>usuario</th>\n",
       "      <th>fecha</th>\n",
       "      <th>polaridad</th>\n",
       "      <th>tópicos</th>\n",
       "      <th>texto procesado</th>\n",
       "      <th>texto stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Salgo de #VeoTV , que día más largoooooo...</td>\n",
       "      <td>142389495503925248</td>\n",
       "      <td>@ccifuentes</td>\n",
       "      <td>2011-12-02T00:47:55</td>\n",
       "      <td>NONE</td>\n",
       "      <td>[\"otros\"]</td>\n",
       "      <td>salgo _HASHTAG_   día largoo</td>\n",
       "      <td>salg _hashtag_ dia largo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@PauladeLasHeras No te libraras de ayudar me/n...</td>\n",
       "      <td>142389933619945473</td>\n",
       "      <td>@CarmendelRiego</td>\n",
       "      <td>2011-12-02T00:49:40</td>\n",
       "      <td>NEU</td>\n",
       "      <td>[\"otros\"]</td>\n",
       "      <td>_USR_ libraras ayudar  besos gracias</td>\n",
       "      <td>_usr_ libr ayud bes graci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>142391947707940864</td>\n",
       "      <td>@CarmendelRiego</td>\n",
       "      <td>2011-12-02T00:57:40</td>\n",
       "      <td>P</td>\n",
       "      <td>[\"otros\"]</td>\n",
       "      <td>_USR_ gracias mar</td>\n",
       "      <td>_usr_ graci mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>142416095012339712</td>\n",
       "      <td>@mgilguerrero</td>\n",
       "      <td>2011-12-02T02:33:37</td>\n",
       "      <td>N+</td>\n",
       "      <td>[\"política\", \"economía\"]</td>\n",
       "      <td>off pensando regalito sinde  va sgae van corru...</td>\n",
       "      <td>off pens regalit sind va sga van corrupt inten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>142422495721562112</td>\n",
       "      <td>@paurubio</td>\n",
       "      <td>2011-12-02T02:59:03</td>\n",
       "      <td>P+</td>\n",
       "      <td>[\"otros\"]</td>\n",
       "      <td>conozco alguien q adicto drama ja ja ja suena d</td>\n",
       "      <td>conozc algui q adict dram ja ja ja suen d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto                  id  \\\n",
       "0        Salgo de #VeoTV , que día más largoooooo...  142389495503925248   \n",
       "1  @PauladeLasHeras No te libraras de ayudar me/n...  142389933619945473   \n",
       "2                          @marodriguezb Gracias MAR  142391947707940864   \n",
       "3  Off pensando en el regalito Sinde, la que se v...  142416095012339712   \n",
       "4  Conozco a alguien q es adicto al drama! Ja ja ...  142422495721562112   \n",
       "\n",
       "           usuario                fecha polaridad                   tópicos  \\\n",
       "0      @ccifuentes  2011-12-02T00:47:55      NONE                 [\"otros\"]   \n",
       "1  @CarmendelRiego  2011-12-02T00:49:40       NEU                 [\"otros\"]   \n",
       "2  @CarmendelRiego  2011-12-02T00:57:40         P                 [\"otros\"]   \n",
       "3    @mgilguerrero  2011-12-02T02:33:37        N+  [\"política\", \"economía\"]   \n",
       "4        @paurubio  2011-12-02T02:59:03        P+                 [\"otros\"]   \n",
       "\n",
       "                                     texto procesado  \\\n",
       "0                    salgo _HASHTAG_   día largoo      \n",
       "1               _USR_ libraras ayudar  besos gracias   \n",
       "2                                  _USR_ gracias mar   \n",
       "3  off pensando regalito sinde  va sgae van corru...   \n",
       "4    conozco alguien q adicto drama ja ja ja suena d   \n",
       "\n",
       "                                          texto stem  \n",
       "0                           salg _hashtag_ dia largo  \n",
       "1                          _usr_ libr ayud bes graci  \n",
       "2                                    _usr_ graci mar  \n",
       "3  off pens regalit sind va sga van corrupt inten...  \n",
       "4          conozc algui q adict dram ja ja ja suen d  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>id</th>\n",
       "      <th>usuario</th>\n",
       "      <th>fecha</th>\n",
       "      <th>texto procesado</th>\n",
       "      <th>texto stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Portada 'Público', viernes. Fabra al banquillo...</td>\n",
       "      <td>142378325086715906</td>\n",
       "      <td>@jesusmarana</td>\n",
       "      <td>2011-12-02T00:03:32</td>\n",
       "      <td>portada público  viernes  fabra banquillo orde...</td>\n",
       "      <td>port public viern fabr banquill orden suprem w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Grande! RT @veronicacalderon \"El periodista es...</td>\n",
       "      <td>142379080808013825</td>\n",
       "      <td>@EvaORegan</td>\n",
       "      <td>2011-12-02T00:06:32</td>\n",
       "      <td>grande rt _USR_ periodista alguien quiere cont...</td>\n",
       "      <td>grand rt _usr_ period algui quier cont realid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Gonzalo Altozano tras la presentación de su li...</td>\n",
       "      <td>142379173120442368</td>\n",
       "      <td>@LosadaPescador</td>\n",
       "      <td>2011-12-02T00:06:55</td>\n",
       "      <td>gonzalo altozano tras presentación libro 101 e...</td>\n",
       "      <td>gonzal altozan tras present libr 101 español d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Mañana en Gaceta: TVE, la que pagamos tú y yo,...</td>\n",
       "      <td>142379815708803072</td>\n",
       "      <td>@mgilguerrero</td>\n",
       "      <td>2011-12-02T00:09:28</td>\n",
       "      <td>mañana gaceta tve  pagamos  culpa becaria fals...</td>\n",
       "      <td>mañan gacet tve pag culp becari fals inform ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Qué envidia “@mfcastineiras: Pedro mañana x la...</td>\n",
       "      <td>142381190123499520</td>\n",
       "      <td>@pedroj_ramirez</td>\n",
       "      <td>2011-12-02T00:14:55</td>\n",
       "      <td>envidia _USR_ pedro mañana x mañana voy paris ...</td>\n",
       "      <td>envidi _usr_ pedr mañan x mañan voy paris alme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto                  id  \\\n",
       "0  Portada 'Público', viernes. Fabra al banquillo...  142378325086715906   \n",
       "1  Grande! RT @veronicacalderon \"El periodista es...  142379080808013825   \n",
       "2  Gonzalo Altozano tras la presentación de su li...  142379173120442368   \n",
       "3  Mañana en Gaceta: TVE, la que pagamos tú y yo,...  142379815708803072   \n",
       "4  Qué envidia “@mfcastineiras: Pedro mañana x la...  142381190123499520   \n",
       "\n",
       "           usuario                fecha  \\\n",
       "0     @jesusmarana  2011-12-02T00:03:32   \n",
       "1       @EvaORegan  2011-12-02T00:06:32   \n",
       "2  @LosadaPescador  2011-12-02T00:06:55   \n",
       "3    @mgilguerrero  2011-12-02T00:09:28   \n",
       "4  @pedroj_ramirez  2011-12-02T00:14:55   \n",
       "\n",
       "                                     texto procesado  \\\n",
       "0  portada público  viernes  fabra banquillo orde...   \n",
       "1  grande rt _USR_ periodista alguien quiere cont...   \n",
       "2  gonzalo altozano tras presentación libro 101 e...   \n",
       "3  mañana gaceta tve  pagamos  culpa becaria fals...   \n",
       "4  envidia _USR_ pedro mañana x mañana voy paris ...   \n",
       "\n",
       "                                          texto stem  \n",
       "0  port public viern fabr banquill orden suprem w...  \n",
       "1  grand rt _usr_ period algui quier cont realid ...  \n",
       "2  gonzal altozan tras present libr 101 español d...  \n",
       "3  mañan gacet tve pag culp becari fals inform ci...  \n",
       "4  envidi _usr_ pedr mañan x mañan voy paris alme...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "usuarios_re = re.compile(r\"@[\\w\\d]+\")\n",
    "hashtags_re = re.compile(r\"#[\\w\\d]+\")\n",
    "email_re = re.compile(r\"\\b[\\w][\\w\\.-]*@\\w[\\w\\.-]*\\.[a-zA-Z]{2,6}\\b\")\n",
    "url_re = re.compile(r\"\\b(\\w+:\\/{2})?[\\d\\w-]+(\\.[\\d\\w-]+)+(/\\S+)*\\b\") \n",
    "remplaza_por_espacios_re = re.compile('[\\n/(){}\\[\\]\\|@,;\\.]')\n",
    "simbolos_a_eliminar_re = re.compile('[^\\d\\w #+_]')\n",
    "palabras_paro = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "def prepara_texto(texto):\n",
    "    text = texto.lower()\n",
    "    \n",
    "    # Codificaciones (problemas con UTF-8, latin1, etc...)\n",
    "    text = re.sub(r'\\\\\\\\', r'\\\\', text)\n",
    "    text = re.sub(r'\\\\\\\\', r'\\\\', text)\n",
    "    text = re.sub(r'\\\\x\\w{2,2}', ' ', text)\n",
    "    text = re.sub(r'\\\\u\\w{4,4}', ' ', text)\n",
    "    text = re.sub(r'\\\\n', ' . ', text)\n",
    "\n",
    "    # Cambia e_mails, urls y usuarios por palabra clave\n",
    "    text = re.sub(email_re, '_EMAIL_', text)\n",
    "    text = re.sub(url_re, '_URL_', text)\n",
    "    text = re.sub(usuarios_re, '_USR_', text)\n",
    "    text = re.sub(hashtags_re, '_HASHTAG_', text)\n",
    "    \n",
    "    # Las palabras con letras repetidas más de 3 veces \n",
    "    # (dos veces por las personas que abusan demasiado)\n",
    "    text = re.sub(r'([a-zA-Z])\\1\\1+(\\w*)', r'\\1\\1\\2', text)\n",
    "    text = re.sub(r'([a-zA-Z])\\1\\1+(\\w*)', r'\\1\\1\\2', text)\n",
    "    \n",
    "    # Elimina simbolos\n",
    "    text = re.sub(remplaza_por_espacios_re, ' ', text)\n",
    "    text = re.sub(simbolos_a_eliminar_re, '', text)\n",
    "    \n",
    "    # Palabras de paro\n",
    "    text = [palabra for palabra in text.split(' ') if palabra not in palabras_paro]\n",
    "    return ' '.join(text)\n",
    "\n",
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "stemmer = nltk.stem.snowball.SpanishStemmer()\n",
    "def stem_texto(texto):\n",
    "    return ' '.join(stemmer.stem(token) for token in tokenizer.tokenize(texto))\n",
    "\n",
    "df_train['texto procesado'] = df_train['texto'].apply(prepara_texto)\n",
    "df_train['texto stem'] = df_train['texto procesado'].apply(stem_texto)\n",
    "display(df_train.head())\n",
    "\n",
    "df_test['texto procesado'] = df_test['texto'].apply(prepara_texto)\n",
    "df_test['texto stem'] = df_test['texto procesado'].apply(stem_texto)\n",
    "display(df_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NONE', 'NEU', 'P', 'N+', 'P+', 'N'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aqui se puede escoger entre 'texto', 'texto procesado' y 'texto stem'\n",
    "columna = 'texto stem'\n",
    "x_train = df_train[columna].values\n",
    "x_test = df_test[columna].values\n",
    "\n",
    "y_polaridad = df_train['polaridad'].values\n",
    "y_topico = df_train['tópicos'].apply(literal_eval).values\n",
    "\n",
    "df_train.polaridad.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Extracción de características\n",
    "\n",
    "\n",
    "La extracción de características, sin un método de entrenamiento, solamente puede hacerse con una matriz de características dispersas de dimensión $N \\times V$, donde $N$ es el número de documentos y $V$ es el tamaño del vocabulario. Esto es debido al desconocimiento de las relaciones de las palabras entre sí. Esta no solo es una matriz gigantésca, si no que aparte la gran mayoría de las entradas de la matriz van a ser cero.\n",
    "\n",
    "El método de base es el conocido como *bolsa de palabras (BOW por Bag of words)*, en el cual se asigna un valor de 1 a cada palabra que se encuentre *al menos* una vez en un documento. Como el uso de palabras solas implica que no existe ninguna relación entre palabras, la ampliación del vocabulario a bigramas, trigramas, o $n$-gramas de cualquier orden, permite conservar en cierta forma las relaciones secuenciales más simples, pero a costa de un incremento en forma exponencial de nuestro vocabulario. Vamos a extraer características tanto con unigramas únicamente como con bigramas y unigramas. ¿Qué pasa si se mueven algunos parámetros como `min_df` o `token_pattern`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El objeto x_train_bow1 es del tipo<class 'scipy.sparse.csr.csr_matrix'>\n",
      "La dimensión de x_train_bow1 es (7218, 2133)\n",
      "La dimensión de x_train_bow2 es (7218, 2672)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bog_unigramas_vectorizer = CountVectorizer(\n",
    "    analyzer='word',        #  Separa por palabras (puede ser por caracteres)\n",
    "    binary=True,            #  BOG (si False, entonces cuenta numero de ocurrencias por documento)\n",
    "    ngram_range=(1,1),      #  n-gramas (min, max) a utilizarse, en este caso solo unigramas\n",
    "    token_pattern=r'(\\S+)', #  Patron de tokenización. Por default solo letras y al menos dos letras\n",
    "    min_df=5,               #  Número de documentos (o proporción) mínimo donde un termino valido aparece\n",
    "    max_df=0.9,             #  Número de documentos (o proporción) máximo donde un termino válido aparece\n",
    ")\n",
    "\n",
    "bog_bigramas_vectorizer = CountVectorizer(\n",
    "    binary=True, \n",
    "    ngram_range=(1,2), \n",
    "    token_pattern=r'(\\S+)', \n",
    "    min_df=5, \n",
    "    max_df=0.9\n",
    ")\n",
    "\n",
    "x_train_bow1 = bog_unigramas_vectorizer.fit_transform(x_train)\n",
    "x_text_bow1 = bog_unigramas_vectorizer.transform(x_test)\n",
    "\n",
    "x_train_bow2 = bog_bigramas_vectorizer.fit_transform(x_train)\n",
    "x_test_bow2 = bog_bigramas_vectorizer.transform(x_test)\n",
    "\n",
    "print(\"El objeto x_train_bow1 es del tipo{}\".format(type(x_train_bow1)))\n",
    "print(\"La dimensión de x_train_bow1 es {}\".format(x_train_bow1.shape))\n",
    "print(\"La dimensión de x_train_bow2 es {}\".format(x_train_bow2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf–idf) (*term frequency–inverse document frequency*) es una medida de la importancia discriminante de una palabra en un documento perteneciente a un corpus. Esta cantidad se forma por dos componentes:\n",
    "\n",
    "* *TF* (frecuencia del término), describe qué tanto se emplea un término en un documento. La lógica de este componente es que mientras más veces aparece un término en un documento, más representativo es del mismo. Existen diversa maneras de calcular este valor, siendo la más simple el conteo directo del término en el documento. La mayoría de las opciones, sin embargo, utilizan alguna variante basada en la frecuencia relativa del término en el documento. \n",
    "\n",
    "$$\n",
    "tf(t, d) = \\frac{\\textrm{Número de veces que aparece el término }t\\textrm{ en el documento }d}\n",
    "{\\textrm{Número de términos en el documento }d} \n",
    "$$\n",
    "\n",
    "* *IDF* (frecuencia inversa del documento), representa la frecuencia con que es empleado el término en el corpus. Un término que es utilizado con mucha frecuencia en el contexto, es poco  discriminante. La forma básica de calcular esta cantidad es como el inverso de la frecuencia de documentos en que aparece el término (escalado logarítmicamente).\n",
    "\n",
    "$$\n",
    "idf(t, D) = \\log\\frac{ 1 + \\textrm{Número de documentos en el corpus }D}\n",
    "{1 + \\textrm{Número de documentos en el corpus }D\\textrm{ donde aparece el término t}} + 1\n",
    "$$\n",
    "\n",
    "El valor de TF para un término/palabra es específico para cada documento, mientras que IDF es un valor global del término en el corpus. El valor de TF-IDF se obtiene multiplicando los valores de TF e IDF:\n",
    "\n",
    "$$\n",
    "tf\\_idf(t, d, D) = tf(t, d)\\cdot idf(t, D)\n",
    "$$\n",
    "y al final cada renglón es normalizado respecto a todos los términos, de forma que cada documento sea representado por un vector de características con norma unitaria (lo que es particularmente interesante para el entrenamiento de muchos modelos de aprendizaje automático).\n",
    "\n",
    "Este método de extracción de características es una modificación al BOG y por lo tanto, la clase para el vectorizador TF-IDF es básicamente la misma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El objeto x_train_tfidf1 es del tipo<class 'scipy.sparse.csr.csr_matrix'>\n",
      "La dimensión de x_train_tfidf1 es (7218, 2071)\n",
      "La dimensión de x_train_tfidf2 es (7218, 2626)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_unigramas_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,1),      #  n-gramas (min, max) a utilizarse, en este caso solo unigramas\n",
    "    token_pattern=r'[a-zñáéíóúü]+', #  Patron de tokenización. Por default solo letras y al menos dos letras\n",
    "    min_df=5,               #  Número de documentos (o proporción) mínimo donde un termino valido aparece\n",
    "    max_df=0.9,             #  Número de documentos (o proporción) máximo donde un termino válido aparece\n",
    "    norm='l2'               #  Norma del vector de cada palabra, lo mñas típico es la euclidiana ('l2')\n",
    ")\n",
    "\n",
    "tfidf_bigramas_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,2), \n",
    "    token_pattern=r'[a-zñáéíóúü]+',\n",
    "    min_df=5, \n",
    "    max_df=0.9\n",
    ")\n",
    "\n",
    "x_train_tfidf1 = tfidf_unigramas_vectorizer.fit_transform(x_train)\n",
    "x_text_tfidf1 = tfidf_unigramas_vectorizer.transform(x_test)\n",
    "\n",
    "x_train_tfidf2 = tfidf_bigramas_vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf2 = tfidf_bigramas_vectorizer.transform(x_test)\n",
    "\n",
    "print(\"El objeto x_train_tfidf1 es del tipo{}\".format(type(x_train_tfidf1)))\n",
    "print(\"La dimensión de x_train_tfidf1 es {}\".format(x_train_tfidf1.shape))\n",
    "print(\"La dimensión de x_train_tfidf2 es {}\".format(x_train_tfidf2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir del objeto vectorizador generado (en esta caso `tfidf_bigramas_vectorizer`) se puede obtener información adicional. En particular es importante conocer a que token corresponde cada una de las columnas y viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La palabra en la posición 1 es \"aa\"\n",
      "La palabra en la posición 3 es \"abc\"\n",
      "La palabra en la posición 100 es \"alumn\"\n",
      "La palabra en la posición 1000 es \"for\"\n",
      "La palabra en la posición 2000 es \"raul\"\n",
      "\n",
      "La palabra \"amor\" se encuentra en la posición 114\n",
      "\n",
      "El documento 442 es:\n",
      "\t\"capaz mejor cancion tant vec oi cant mejor maner record quier _url_\"\n",
      "y los 3 tokens con mayor valor de tf-idf son:\n",
      "\tmejor\t\tvalor de ttf-idf: 0.4522\n",
      "\tcapaz\t\tvalor de ttf-idf: 0.3744\n",
      "\tcancion\t\tvalor de ttf-idf: 0.3290\n"
     ]
    }
   ],
   "source": [
    "vocabulario = tfidf_bigramas_vectorizer.get_feature_names()\n",
    "\n",
    "for i in [1, 3, 100, 1000, 2000]:\n",
    "    print('La palabra en la posición {} es \"{}\"'.format(i, vocabulario[i]))\n",
    "# display(vocabulario)\n",
    "\n",
    "print('\\nLa palabra \"amor\" se encuentra en la posición {}'.format(tfidf_bigramas_vectorizer.vocabulary_.get('amor')))\n",
    "\n",
    "numdoc = np.random.randint(0, 1500)\n",
    "a = x_train_tfidf2[numdoc, :].toarray().ravel()\n",
    "mejores = [(vocabulario[i], a[i]) for i in a.argsort()[-1:-4:-1]]\n",
    "\n",
    "print('\\nEl documento {} es:\\n\\t\"{}\"'.format(numdoc, x_train[numdoc]))\n",
    "print('y los 3 tokens con mayor valor de tf-idf son:')\n",
    "for (token, tfidf) in mejores:\n",
    "    print(\"\\t{}\\t\\tvalor de ttf-idf: {:06.4f}\".format(token, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, para la clasificación de textos el método de TF-IDF da mejores resultados que BOW, y su complejidad computacional es muy similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Clasificación de documentos\n",
    "\n",
    "En esta sección vamos a mostrar dos formas diferentes de clasificar documentos, utilizando los métodos disponibles en *sklearn*: Una para una clasificación en la que cada documento del conjunto de aprendizaje se encuentra etiquetado en una sola clase (como es el caso de la polaridad, para el análisis de sentimientos), y el caso en el cual cada documento puede estar asignado a una o más etiquetas (como es el caso de los tópicos).\n",
    "\n",
    "Al estar fuera del alcance de este curso la discusión sobre los métodos de aprendizaje supervisado, vamos aretener solamente uno de los métodos más clásicos (regresión logística). Les recomiendo explorar otros métodos, en particular las máquinas de vectores de soporte para clasificación y los bosques aleatorios. Estos dos métodos suelen obtener muy buenos resultados en los diferentes *benchmarks* y competencias.\n",
    "\n",
    "### 3.2.1 Análisis de sentimientos (polaridad)\n",
    "\n",
    "Comencemos por el principio, que es separar los datos de entrenamiento de los de validación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento = 6496.        Tamaño del conjunto de validación = 722.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_entrena, x_valida, y_entrena, y_valida = train_test_split(\n",
    "    x_train_tfidf2, \n",
    "    y_polaridad, \n",
    "    test_size=0.1, \n",
    "    random_state=10\n",
    ")\n",
    "print('Tamaño del conjunto de entrenamiento = {}. \\\n",
    "       Tamaño del conjunto de validación = {}.'.format(x_entrena.shape[0], x_valida.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a generar un objeto clasificador, con un [clasificador logístico](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), y realizamos el entrenamiento. Para más información sobre la regresión logística un buen tutorial se puede encontrar [aquí (capítulo 2, pero el capítulo 1 tambien está muy bueno](http://cs229.stanford.edu/notes/cs229-notes1.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='ovr', n_jobs=None, penalty='l2', random_state=1,\n",
       "                   solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    penalty='l2',          # Norma a seguir en la optimización ('l2' o 'l1')\n",
    "    C=1.0,                 # Inverso a la constante de regularización, para mejorar la generalización\n",
    "    random_state=1,        # Solo para poder tener repetibilidad (muy útil para debuguear)\n",
    "    multi_class='ovr'      # Para más de dos clases ('ovr' para one-vs-all, 'multinomial' para softmax)\n",
    ")\n",
    "\n",
    "clf.fit(x_entrena, y_entrena)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a ver los resultados, de acuerdo a los [indicadores clásicos](https://en.wikipedia.org/wiki/Precision_and_recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Para los datos de entrenamiento\n",
      "========================================\n",
      "\n",
      "Porcentaje de acierto: 0.6617918719211823\n",
      "\n",
      "Precisión, recall y f1-score\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.59      0.75      0.66      1201\n",
      "          N+       0.78      0.55      0.64       764\n",
      "         NEU       0.84      0.20      0.33       597\n",
      "        NONE       0.65      0.83      0.73      1328\n",
      "           P       0.70      0.47      0.57      1109\n",
      "          P+       0.67      0.83      0.74      1497\n",
      "\n",
      "    accuracy                           0.66      6496\n",
      "   macro avg       0.70      0.60      0.61      6496\n",
      "weighted avg       0.68      0.66      0.64      6496\n",
      "\n",
      "\n",
      "\n",
      "Para los datos de validación\n",
      "========================================\n",
      "\n",
      "Porcentaje de acierto: 0.4196675900277008\n",
      "\n",
      "Precisión, recall y f1-score\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.33      0.50      0.40       134\n",
      "          N+       0.35      0.20      0.26        83\n",
      "         NEU       0.38      0.07      0.12        73\n",
      "        NONE       0.50      0.56      0.53       154\n",
      "           P       0.30      0.22      0.25       123\n",
      "          P+       0.52      0.65      0.58       155\n",
      "\n",
      "    accuracy                           0.42       722\n",
      "   macro avg       0.40      0.37      0.36       722\n",
      "weighted avg       0.41      0.42      0.40       722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def reporte(clasificador, x, t, labels=None):\n",
    "    y = clasificador.predict(x)\n",
    "    print(\"\\nPorcentaje de acierto: {}\".format(accuracy_score(t, y)))\n",
    "    print(\"\\nPrecisión, recall y f1-score\")\n",
    "    print(classification_report(t, y, target_names=labels))    \n",
    "\n",
    "print(\"\\n\\nPara los datos de entrenamiento\\n\" + 40*\"=\")\n",
    "reporte(clf, x_entrena, y_entrena)\n",
    "print(\"\\n\\nPara los datos de validación\\n\" + 40*\"=\")\n",
    "reporte(clf, x_valida, y_valida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los [resultados reportados](http://ceur-ws.org/Vol-1397/overview.pdf) con el conjunto de prueba (del que no disponemos los datos) se encuentran distribuidos alrededor del 0.5 de porcentaje de acierto. Es interesante notar como nuestros resultados no son tan alejados (en un primer intento) de los presentados en trabajos que hacen uso de tecnicas extremadamente sofisticadas, como son las [redes neuronales profundas recurrentes](http://ceur-ws.org/Vol-1397/lys.pdf). Sin embargo, no estamos conformes con los resultados y debemos de hacer una mejor clasificación, lo que nos lleva a la *tarea 1*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, es importante la interpretabilidad del modelo. Dado que en la regresión logística se agrega la información de cada una de las características de forma lineal, es posible ver a los coeficientes como una medida de peso de la importancia (positiva y negativa) que tienen cada uno de los *tokens* para cada una de las etiquetas. Veamos cuales son las palabras con más peso positivo y con más peso negativo para una etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las clases disponibles son: ['N', 'N+', 'NEU', 'NONE', 'P', 'P+']\n",
      "Los tokens más positivos para la clase N+ son:\n",
      "\t- deficit                  (peso: 3.423)\n",
      "\t- recort                   (peso: 3.394)\n",
      "\t- corrupcion               (peso: 3.275)\n",
      "\t- par                      (peso: 3.088)\n",
      "\t- denunci                  (peso: 2.115)\n",
      "\t- vergüenz                 (peso: 1.986)\n",
      "\t- crisis                   (peso: 1.938)\n",
      "\t- desp                     (peso: 1.866)\n",
      "\t- muert                    (peso: 1.853)\n",
      "\t- tram                     (peso: 1.837)\n",
      "Los tokens más negativos para la clase son:\n",
      "\t- buen                     (peso:-2.874)\n",
      "\t- graci                    (peso:-2.001)\n",
      "\t- usr                      (peso:-1.716)\n",
      "\t- port                     (peso:-1.608)\n",
      "\t- hoy                      (peso:-1.573)\n",
      "\t- noch                     (peso:-1.362)\n",
      "\t- hashtag                  (peso:-1.338)\n",
      "\t- quier                    (peso:-1.336)\n",
      "\t- url                      (peso:-1.276)\n",
      "\t- feliz                    (peso:-1.252)\n"
     ]
    }
   ],
   "source": [
    "def tokens_principales(clasificador, clase, vocabulario):\n",
    "    clase_ind = clasificador.classes_.tolist().index(clase)\n",
    "    pesos = clasificador.coef_[clase_ind,:]\n",
    "    indices = pesos.argsort()\n",
    "    mejores = [(vocabulario[i], pesos[i]) for i in indices[-1:-11:-1]]\n",
    "    peores = [(vocabulario[i], pesos[i]) for i in indices[:10]]\n",
    "    \n",
    "    return mejores, peores\n",
    "\n",
    "clase_sel = 'N+'\n",
    "mejores, peores = tokens_principales(clf, clase_sel, vocabulario)\n",
    "\n",
    "print(\"Las clases disponibles son: {}\".format(clf.classes_.tolist()))\n",
    "print(\"Los tokens más positivos para la clase {} son:\".format(clase_sel))\n",
    "for (t, w) in mejores:\n",
    "    print(\"\\t- {:25}(peso:{:6.4})\".format(t,w))\n",
    "print(\"Los tokens más negativos para la clase son:\")\n",
    "for (t, w) in peores:\n",
    "    print(\"\\t- {:25}(peso:{:6.4})\".format(t,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, veamos que encuentra con el conjunto de prueba, revisando algunos ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = clf.predict(x_test_tfidf2)\n",
    "\n",
    "for i in [1, 10, 100, 500]:\n",
    "    print(\"Twwet:\\t {}\".format(df_test.loc[i,'texto']))\n",
    "    print('Polaridad estimada: {}'.format(y_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Clasificación de tópicos\n",
    "\n",
    "Ahora vamos a realizar la clasificación de tópicos, la cual es una tarea más dificil que el análisis de sentimientos, al estár más clases involucradas, y al tener varias etquetas por cada documento. Para esto vamos a utilizar los métodos del módulo *sklearn*. Vamos a preparar los datos para realizar la tarea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Para los datos de entrenamiento\n",
      "========================================\n",
      "\n",
      "Porcentaje de acierto: 0.6134544334975369\n",
      "\n",
      "Precisión, recall y f1-score\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "           cine       0.92      0.30      0.46       220\n",
      "       deportes       1.00      0.25      0.41       102\n",
      "       economía       0.94      0.70      0.81       855\n",
      "entretenimiento       0.91      0.56      0.69      1531\n",
      "         fútbol       0.98      0.43      0.59       225\n",
      "     literatura       1.00      0.35      0.52        89\n",
      "         música       0.97      0.51      0.67       505\n",
      "          otros       0.87      0.76      0.81      2086\n",
      "       política       0.94      0.88      0.91      2802\n",
      "     tecnología       0.97      0.30      0.46       191\n",
      "\n",
      "      micro avg       0.92      0.70      0.80      8606\n",
      "      macro avg       0.95      0.50      0.63      8606\n",
      "   weighted avg       0.92      0.70      0.78      8606\n",
      "    samples avg       0.79      0.72      0.74      8606\n",
      "\n",
      "\n",
      "\n",
      "Para los datos de validación\n",
      "========================================\n",
      "\n",
      "Porcentaje de acierto: 0.40304709141274236\n",
      "\n",
      "Precisión, recall y f1-score\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "           cine       0.50      0.12      0.19        25\n",
      "       deportes       1.00      0.09      0.17        11\n",
      "       economía       0.73      0.34      0.47        87\n",
      "entretenimiento       0.56      0.32      0.41       147\n",
      "         fútbol       0.83      0.19      0.30        27\n",
      "     literatura       0.50      0.07      0.12        14\n",
      "         música       0.84      0.34      0.49        61\n",
      "          otros       0.69      0.56      0.62       250\n",
      "       política       0.84      0.73      0.78       318\n",
      "     tecnología       0.67      0.08      0.14        26\n",
      "\n",
      "      micro avg       0.74      0.50      0.60       966\n",
      "      macro avg       0.72      0.28      0.37       966\n",
      "   weighted avg       0.73      0.50      0.57       966\n",
      "    samples avg       0.59      0.53      0.54       966\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Se procesa las etiquetas en forma de multietiquetas\n",
    "topico_count = {}\n",
    "for y in y_topico:\n",
    "    for topico in y:\n",
    "        topico_count[str(topico)] = topico_count.setdefault(topico, 0) + 1    \n",
    "mlb = MultiLabelBinarizer(classes=sorted(topico_count.keys()))\n",
    "y_topico_mlb = mlb.fit_transform(y_topico)\n",
    "\n",
    "# Se separan los datos en conjunto de entrenamiento y validación\n",
    "x_ent_mlb, x_val_mlb, y_ent_mlb, y_val_mlb = train_test_split(\n",
    "    x_train_tfidf2, \n",
    "    y_topico_mlb, \n",
    "    test_size=0.1, \n",
    "    random_state=10\n",
    ")\n",
    "\n",
    "# Se crea un clasificador para multiples etiquetas\n",
    "clf_mlb = OneVsRestClassifier(\n",
    "    LogisticRegression(\n",
    "        C=5.0,\n",
    "        penalty='l2'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Se entrena el clasificador\n",
    "clf_mlb.fit(x_ent_mlb, y_ent_mlb)\n",
    "\n",
    "# Se revisa los indices de desempeño\n",
    "print(\"\\n\\nPara los datos de entrenamiento\\n\" + 40*\"=\")\n",
    "reporte(clf_mlb, x_ent_mlb, y_ent_mlb, mlb.classes)\n",
    "print(\"\\n\\nPara los datos de validación\\n\" + 40*\"=\")\n",
    "reporte(clf_mlb, x_val_mlb, y_val_mlb, mlb.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para terminar la sección, revisemos algunos tweets del conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twwet:\t Grande! RT @veronicacalderon \"El periodista es alguien que quiere contar la realidad, pero no vive en ella\" via @galtares\n",
      "Tópicos estimados: ('entretenimiento',)\n",
      "Twwet:\t Definitivamente, creo que me he resfriado. Con este tiempo de locos que ha estado haciendo estos meses, ahora toca las consecuencias.\n",
      "Tópicos estimados: ('política',)\n",
      "Twwet:\t No hubo tiempo para sacar a Franco del Valle de los Caídos pero sí lo ha habido para ampliar la base militar de Rota.\n",
      "Tópicos estimados: ('política',)\n",
      "Twwet:\t Curiosa la evolución del twitter de @_rubalcaba_ ¿Seguimos en jornada de reflexión? http://t.co/i1btoFHo\n",
      "Tópicos estimados: ('entretenimiento',)\n",
      "Twwet:\t Venga y vamos a apuntarnos todos RT @FotosYTuits: Ya hay fecha!!!! Marcar el jueves 15 de diciembre #fyt06 http://t.co/KgXvI7fI\n",
      "Tópicos estimados: ()\n",
      "Twwet:\t Vuelvo a tweeter,he estado ausente una semana,me gusta más el mundo de la presencia física,el apretón de manos del amigo,la palabra cálida..\n",
      "Tópicos estimados: ('otros',)\n"
     ]
    }
   ],
   "source": [
    "y_test_mlb = clf_mlb.predict(x_test_tfidf2)\n",
    "y_test_topico = mlb.inverse_transform(y_test_mlb)\n",
    "\n",
    "for i in [1, 10, 100, 500, 1000, 2000]:\n",
    "    print(\"Twwet:\\t {}\".format(df_test.loc[i,'texto']))\n",
    "    print('Tópicos estimados: {}'.format(y_test_topico[i]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
